# -*- coding: utf-8 -*-
"""Final-binary-class.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YQKnLZqvedoeE24sZiocMy-5ya-cCyD5
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from datetime import datetime
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OrdinalEncoder
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.linear_model import LinearRegression,LogisticRegression
from sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier
from sklearn.ensemble import GradientBoostingRegressor,GradientBoostingClassifier
from sklearn.ensemble import AdaBoostRegressor,AdaBoostClassifier
from sklearn.ensemble import ExtraTreesRegressor,ExtraTreesClassifier
from sklearn.neighbors import KNeighborsRegressor,KNeighborsClassifier
from sklearn.svm import SVR,SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score,mean_squared_error
from sklearn.preprocessing import LabelEncoder

!pip install category_encoders
import category_encoders as ce

df = pd.read_csv("/content/FW_Veg_Rem_Combined.csv")
df.head(5)

df.describe(include='all')

df.info()

"""Unnamed: 0.1 - serial numbers, no information


Unnamed: 0 - serial numbers, no information

 ***data is from 1991 - 2015***

fire_name - very few descriptive entries like 'crazy', 'grass fire' that describe the fire but no additional information

fire_size - as we are using classification we would remove this feature to prevent data leakage

fire_size_class - we will reomve it as it is the y or target variable we are trying to predict

stat_cause_descr - need to apply target encoding/labelling

state - as lat/long describe the state we can remove it

lat/long - we need to apply some method to better represent info from these 2 variables as one attribute

disc_date_final, cont_date_final - we remove them as the cleaned dates have extracted the date values from this timestamp entry

disc_clean_date - keep, for month & date we'll apply cyclic encoding

cont_clean_date, putout_time - remove. we don't need the details for the date when the fire was contained as we are trying to predict it


discovery_mont - removed as we will extract it from disc_clean_date

disc_date_pre - keep as it has info 30 days prior to start of fire

disc_pre_month disc_pre_year - remove, will extract from pre date

wstation_usaf, wstation_wban - remove as it is pincode for weather stations and does not provide additional info since we have lat/long details for fire areas

dstation_m - we can remove it as size of the fire is not related to how far the weather station is from it

wstation_byear', 'wstation_eyear - keep both

vegetation - need to apply target encoding/labelling

fire_mag - remove as it is the same as fire_size and we are classifying so it's a redundant feature

weather_file - half of them are mising and doesn't provide any valuable info

'Temp_pre_30', 'Temp_pre_15',
       'Temp_pre_7', 'Temp_cont', 'Wind_pre_30', 'Wind_pre_15', 'Wind_pre_7',
       'Wind_cont', 'Hum_pre_30', 'Hum_pre_15', 'Hum_pre_7', 'Hum_cont',
       'Prec_pre_30', 'Prec_pre_15', 'Prec_pre_7', 'Prec_cont', 'remoteness' - keep and check model performace with/without values for 15 days pre and 30 days pre

"""

df = df.drop(['Unnamed: 0.1', 'Unnamed: 0', 'fire_name', 'fire_size', 'state', 'cont_clean_date',
         'discovery_month', 'disc_date_final', 'cont_date_final', 'putout_time', 'disc_pre_year', 'disc_pre_month',
         'wstation_usaf', 'dstation_m', 'wstation_wban', 'fire_mag', 'weather_file'],axis=1)

df.head(10)

df.info()

df['fire_size_class'].value_counts()

class_mapping = {'D':'C', 'E':'C', 'F':'C', 'G':'C'}
df = df.replace(class_mapping)

df['fire_size_class'].value_counts()

# Extract day, month, year from discovery clean date
df['disc_clean_date'] = pd.to_datetime(df['disc_clean_date'])

# df['disc_day'] = df['disc_clean_date'].dt.day
df['disc_month'] = df['disc_clean_date'].dt.month
df['disc_year'] = df['disc_clean_date'].dt.year

# Extract if discovery clean date was weekend or not
# df["disc_day_of_Week"] = df['disc_clean_date'].dt.dayofweek
# df["disc_is_Weekend"] = df['disc_clean_date'].dt.dayofweek > 4

# df['disc_is_Weekend'] = df['disc_is_Weekend'].astype(int)

# # Extract day, month, year from discovery previous date
# df['disc_date_pre'] = pd.to_datetime(df['disc_date_pre'])

# df['disc_day_pre'] = df['disc_date_pre'].dt.day
# df['disc_month_pre'] = df['disc_date_pre'].dt.month
# df['disc_year_pre'] = df['disc_date_pre'].dt.year

# # Function to determine the season based on disc_month
# def get_season(row):
#     month = row['disc_month']
#     if 0 < month <= 3:
#         return 'Winter'
#     elif 3 < month <= 6:
#         return 'Spring'
#     elif 6 < month <= 9:
#         return 'Summer'
#     elif 9 < month <= 12:
#         return 'Fall'

# df['season'] = df.apply(get_season, axis=1)

# # Apply One Hot encoding the season column
# df_season_encoded = pd.get_dummies(df['season'], prefix='season')
# df = pd.concat([df, df_season_encoded], axis=1)

# Apply Cyclic Feature Encoding to the month and day columns
def encode(df, col, max_val):
    df[col + '_sin'] = np.sin(2 * np.pi * df[col]/max_val)
    df[col + '_cos'] = np.cos(2 * np.pi * df[col]/max_val)
    return df

df = encode(df, 'disc_month', 12)
# df = encode(df, 'disc_day', 31)

# df = encode(df, 'disc_month_pre', 12)
# df = encode(df, 'disc_day_pre', 31)

# df['x'] = np.cos(df['latitude']) * np.cos(df['longitude'])
# df['y'] = np.cos(df['latitude']) * np.sin(df['longitude'])
# df['z'] = np.sin(df['latitude'])

# Drop the unrequired columns
df = df.drop(['disc_clean_date', 'disc_date_pre',
              'disc_month', 'wstation_byear', 'wstation_eyear'],axis=1)

df.info()

# df['wstation_byear'] = df['wstation_byear'].astype(object)
# df['wstation_eyear'] = df['wstation_eyear'].astype(object)
df['Vegetation'] = df['Vegetation'].astype(object)
df['disc_year'] = df['disc_year'].astype(object)
# df['disc_year_pre'] = df['disc_year_pre'].astype(object)

df.info()

X = df.drop('fire_size_class',axis=1)
y  = df['fire_size_class']

X.columns

def target_encode_multiclass(X,y): #X,y are pandas df and series
    y=y.astype(str)   #convert to string to onehot encode
    enc=ce.OneHotEncoder().fit(y)
    y_onehot=enc.transform(y)
    class_names=y_onehot.columns  #names of onehot encoded columns
    X_obj=X.select_dtypes('object') #separate categorical columns
    X=X.select_dtypes(exclude='object')
    for class_ in class_names:

        enc=ce.TargetEncoder()
        enc.fit(X_obj,y_onehot[class_]) #convert all categorical
        temp=enc.transform(X_obj)       #columns for class_
        temp.columns=[str(x)+'_'+str(class_) for x in temp.columns]
        X=pd.concat([X,temp],axis=1)    #add to original dataset

    return X

X = target_encode_multiclass(X,y)

X.head()

X.info()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)

classifiers=[['Logistic Regression :',LogisticRegression()],
       ['Decision Tree Classification :',DecisionTreeClassifier()],
       ['Random Forest Classification :',RandomForestClassifier()],
       ['Gradient Boosting Classification :', GradientBoostingClassifier()],
       ['Ada Boosting Classification :',AdaBoostClassifier()],
       ['Extra Tree Classification :', ExtraTreesClassifier()],
       ['K-Neighbors Classification :',KNeighborsClassifier()],
       ['Support Vector Classification :',SVC()],
       ['Gausian Naive Bayes :',GaussianNB()]]
cla_pred=[]
for name,model in classifiers:
    model=model
    model.fit(X_train,y_train)
    predictions = model.predict(X_test)
    cla_pred.append(accuracy_score(y_test,predictions))
    print(name,accuracy_score(y_test,predictions))

"""1. Perform k fold cross validation
2. Print Classification report
3. Try applying Standard scaling on Temp, Hum, Prec, Wind
4. Finalize on ensemble type and algorithms

Logistic Regression : 0.6923424236951418
Decision Tree Classification : 0.679971103485642
Random Forest Classification : 0.7558244536752754
Gradient Boosting Classification : 0.7552826440310637
Ada Boosting Classification : 0.7379447354162905
Extra Tree Classification : 0.7483294202636807
K-Neighbors Classification : 0.6924327253025104
Support Vector Classification : 0.6983023297814701
Gausian Naive Bayes : 0.6798808018782735


Logistic Regression : 0.695322376738306
Decision Tree Classification : 0.6753657215098429
Random Forest Classification : 0.7550117392089579
Gradient Boosting Classification : 0.7551923424236952
Ada Boosting Classification : 0.7379447354162905
Extra Tree Classification : 0.7382156402383963
K-Neighbors Classification : 0.6955029799530431
"""

# Ensemble
from sklearn.metrics import log_loss
from sklearn.ensemble import VotingClassifier

# model_1 = LogisticRegression()
# model_2 = DecisionTreeClassifier()
model_3 = RandomForestClassifier()
model_4 = KNeighborsClassifier()
model_5 = SVC()
# model_6 = GaussianNB()

# Making the final model using voting classifier
final_model = VotingClassifier(
#     estimators=[('lr', model_1), ('dt', model_2), ('rf', model_3),
#                 ('knn', model_4), ('svm', model_5), ('nb', model_6)], voting='hard')
    estimators=[('rf', model_3),
                ('knn', model_4), ('svm', model_5)], voting='hard')

# training all the model on the train dataset
final_model.fit(X_train, y_train)

# predicting the output on the test dataset
pred_final = final_model.predict(X_test)

# printing log loss between actual and predicted value
# print(log_loss(y_test, pred_final))
print('ensemble',accuracy_score(y_test,pred_final))

"""Ensemble accuracies
All 6 - 0.7329781470110168
rf, svm, knn, lr, dt - 0.7383962434531335
rf, svm, knn, lr - 0.7220516525194148
rf, svm, knn - 0.7357774968394437

***Other Ensemble Methods***
"""

#XGBoost
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score

# Create an XGBoost classifier
xgb_clf = xgb.XGBClassifier(n_estimators=100, learning_rate=0.05, random_state=42)
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
y_test = label_encoder.fit_transform(y_test)

# Train the model
xgb_clf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = xgb_clf.predict(X_test)
y_proba = xgb_clf.predict_proba(X_test)[:, 1]  # Probabilities for positive class

# Print accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy: {:.2f}%".format(accuracy * 100))

# Print classification report
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Print confusion matrix
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

# Print ROC-AUC score
roc_auc = roc_auc_score(y_test, y_proba)
print("ROC-AUC Score: {:.4f}".format(roc_auc))

#Stacking
from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC

# Load your dataset and split into features (X) and target variable (y)

# Define base models
model1 = LogisticRegression()
model2 = DecisionTreeClassifier()
model3 = SVC(probability=True)

# Create a stacking classifier with a logistic regression meta-classifier
stacking_clf = StackingClassifier(estimators=[('lr', model1), ('dt', model2), ('svc', model3)], final_estimator=LogisticRegression())

# Evaluate the stacking classifier
scores = cross_val_score(stacking_clf, X_train, y_train)
print("Accuracy: {:.2f}%".format(scores.mean() * 100))

from sklearn.ensemble import BaggingClassifier
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score
# Load your dataset and split into features (X) and target variable (y)

# Create a base model (e.g., Decision Tree)
base_model = DecisionTreeClassifier()

# Create a bagging classifier
bagging_clf = BaggingClassifier(base_model, n_estimators=50, random_state=42)

# Evaluate the bagging classifier
scores = cross_val_score(bagging_clf, X, y)
print("Accuracy: {:.2f}%".format(scores.mean() * 100))

"""

---

"""

# results_df = pd.DataFrame(
#     'models': ['Logistic Regression' ,
#       'Decision Tree Classifier',
#       'Random Forest Classifier',
#       'Gradient Boosting Classifier',
#       'Ada Boosting Classifier',
#       'Extra Tree Classifier' ,
#       'K-Neighbors Classifier',
#       'Support Vector Classifier',
#        'Gaussian Naive Bayes'],
#     'accuracies'
# )

# [0.656763590391909,
#  0.6105291674191801,
#  0.7063391728372765,
#  0.7239479862741557,
#  0.7003792667509482,
#  0.6849376918909157,
#  0.6469207151887304,
#  0.6668773704171934,
#  0.5891276864728192]

# after lat/log as x y z
# [0.6606465595087593,
#  0.5891276864728192,
#  0.6987538378183131,
#  0.7036301246162182,
#  0.6863825176088134,
#  0.6834025645656493,
#  0.6332851724760701,
#  0.6566732887845403,
#  0.588947083258082]

cla_pred

model = LogisticRegression()
model.fit(X_train, y_train)
importance = model.coef_[0]

predictions = model.predict(X_test)
print('LogisticRegression', accuracy_score(y_test,predictions))

imp = []
for c,i in zip(X.columns, importance):
    imp.append([c,i])
imp.sort(key=lambda x: x[1], reverse=True)
for i in imp:
    print(i)
# plt.bar([x for x in range(len(importance))], importance)
# plt.show()


# LogisticRegression 0.656763590391909
# ['stat_cause_descr_fire_size_class_2', 0.12940427328998422]
# ['disc_month_sin', 0.11923825180336535]
# ['disc_month_pre_cos', 0.11127455162167435]
# ['Vegetation_fire_size_class_2', 0.07439103111485108]
# ['disc_month_pre_sin', 0.07426079332197155]

# after lat/log as x y z
# LogisticRegression 0.656763590391909
# ['stat_cause_descr_fire_size_class_2', 0.12940427328998422]
# ['disc_month_sin', 0.11923825180336535]
# ['disc_month_pre_cos', 0.11127455162167435]
# ['Vegetation_fire_size_class_2', 0.07439103111485108]
# ['disc_month_pre_sin', 0.07426079332197155]

model = DecisionTreeClassifier()
model.fit(X_train, y_train)
importance = model.feature_importances_

predictions = model.predict(X_test)
print('DecisionTreeClassifier', accuracy_score(y_test,predictions))

imp = []
for c,i in zip(X.columns, importance):
    imp.append([c,i])
imp.sort(key=lambda x: x[1], reverse=True)

for i in imp[:]:
    print(i)

# DecisionTreeClassifier 0.6063752934802239
# ['remoteness', 0.1354585357832939]
# ['longitude', 0.1012107156775564]
# ['latitude', 0.06877463140083732]
# ['stat_cause_descr_fire_size_class_2', 0.05490681624903567]
# ['disc_day_pre_sin', 0.029295540792649673]

# after lat/log as x y z
# DecisionTreeClassifier 0.5851544157486003
# ['remoteness', 0.12003171604507377]
# ['stat_cause_descr_fire_size_class_6', 0.053766463004961774]
# ['y', 0.05200154417832613]
# ['x', 0.0507083933667336]
# ['z', 0.050107305205507144]

model = RandomForestClassifier()
model.fit(X_train, y_train)
importance = model.feature_importances_

predictions = model.predict(X_test)
print('RandomForestClassifier', accuracy_score(y_test,predictions))

imp = []
for c,i in zip(X.columns, importance):
    imp.append([c,i])
imp.sort(key=lambda x: x[1], reverse=True)

for i in imp[:]:
    print(i)


# RandomForestClassifier 0.7098609355246523
# ['remoteness', 0.09110534223648585]
# ['longitude', 0.06440417671809356]
# ['latitude', 0.046918060649682305]
# ['disc_day_pre_sin', 0.025302422044703524]
# ['disc_day_sin', 0.02513375177121365]

# after lat/log as x y z
# RandomForestClassifier 0.6991150442477876
# ['remoteness', 0.07659103327629359]
# ['z', 0.037063674484370646]
# ['x', 0.036719489644309114]
# ['y', 0.03658808297896333]
# ['disc_day_pre_sin', 0.024054041777592204]

model = GradientBoostingClassifier()
model.fit(X_train, y_train)
importance = model.feature_importances_

predictions = model.predict(X_test)
print('GradientBoostingClassifier', accuracy_score(y_test,predictions))

imp = []
for c,i in zip(X.columns, importance):
    imp.append([c,i])
imp.sort(key=lambda x: x[1], reverse=True)

for i in imp[:]:
    print(i)

# GradientBoostingClassifier 0.723857684666787
# ['remoteness', 0.4241353722722815]
# ['longitude', 0.19429808122867717]
# ['stat_cause_descr_fire_size_class_2', 0.10041507720303922]
# ['latitude', 0.09931038931227348]
# ['Wind_cont', 0.01973412394007389]

# after lat/log as x y z
# GradientBoostingClassifier 0.7039913310456926
# ['remoteness', 0.34206589881804006]
# ['stat_cause_descr_fire_size_class_2', 0.11385042808107278]
# ['stat_cause_descr_fire_size_class_3', 0.07208546791985636]
# ['stat_cause_descr_fire_size_class_6', 0.048419678954450464]
# ['Temp_cont', 0.04366496386002072]

model = AdaBoostClassifier()
model.fit(X_train, y_train)
importance = model.feature_importances_

predictions = model.predict(X_test)
print('AdaBoostClassifier', accuracy_score(y_test,predictions))

imp = []
for c,i in zip(X.columns, importance):
    imp.append([c,i])
imp.sort(key=lambda x: x[1], reverse=True)

for i in imp[:]:
    print(i)


# AdaBoostClassifier 0.7003792667509482
# ['remoteness', 0.32]
# ['longitude', 0.18]
# ['latitude', 0.04]
# ['Wind_cont', 0.04]
# ['Prec_cont', 0.04]

# after lat/log as x y z
# AdaBoostClassifier 0.6863825176088134
# ['remoteness', 0.32]
# ['Wind_cont', 0.04]
# ['Hum_pre_7', 0.04]
# ['Prec_cont', 0.04]
# ['stat_cause_descr_fire_size_class_2', 0.04]



from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Create a Random Forest Classifier
rf_classifier = RandomForestClassifier(random_state=42)

# Use Grid Search with 5-fold cross-validation
grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_

# Train a new Random Forest Classifier with the best hyperparameters
best_rf_classifier = RandomForestClassifier(random_state=42, **best_params)
best_rf_classifier.fit(X_train, y_train)

# Make predictions on the test set
y_pred = best_rf_classifier.predict(X_test)

# Calculate the accuracy score
accuracy = accuracy_score(y_test, y_pred)
print(f"Best Random Forest Classification Accuracy: {accuracy:.4f}")
print(f"Best Hyperparameters: {best_params}")

param_grid = {
    'n_estimators': [25, 50, 100, 150],
    'max_features': ['sqrt', 'log2', None],
    'max_depth': [3, 6, 9],
    'max_leaf_nodes': [3, 6, 9],
}

grid_search = GridSearchCV(RandomForestClassifier(), param_grid=param_grid)
grid_search.fit(X_train, y_train)
print(grid_search.best_estimator_)

model_grid = RandomForestClassifier(grid_search.best_params_)
model_grid.fit(X_train, y_train)
y_pred_grid = model.predict(X_test)
print(classification_report(y_pred_grid, y_test))

random_search = RandomizedSearchCV(RandomForestClassifier(), param_grid)
random_search.fit(X_train, y_train)
print(random_search.best_estimator_)

model_random = RandomForestClassifier(random_search.best_params_)
model_random.fit(X_train, y_train)
y_pred_rand = model.predict(X_test)
print(classification_report(y_pred_rand, y_test))

import geopandas as gpd
from shapely.geometry import Point

# Convert it to a GeoDataFrame by transforming the Latitude/Longitude coordinates
loc_crs = {'init': 'epsg:4326'}
loc_geom = [Point(xy) for xy in zip(df['longitde'], df['latitude'])]
geo_df = gpd.GeoDataFrame(df, crs=loc_crs, geometry=loc_geom)

# Plot the GeoDataFrame
geo_df.plot()



